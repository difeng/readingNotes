## spark数据读取与保存

### 支持的文件格式：

|格式名称|结构化|备注|
|:----:|:----:|:----:|
|文本文件|否|普通文件，每行一条记录|
|JSON|半结构化|常见基于文本的格式，半结构化，大多数库都要求每行一条记录|
|CSV |是|非常常见的基于文本的格式，通常在电子表格中使用|
|SequenceFiles|是|一种用于键值对数据的常见的Hadoop文件格式|
|Protocol buffers|是|一种快速、节约空间的跨语言格式|
|对象文件|是|序列化的spark作业中的数据对象，当类改变时它会失效|

### 读取文本文件
#### scala读取
val input = sc.textFile("file://path")
支持目录和通配符

#### 文本文件保存
result.saveAsTextFile(outputFile)

spark提交任务过程

1.spark-submit提交应用
2.submit脚本启动驱动器程序，调用应用的main方法
3.驱动器与集群管理节点通信，申请资源
4.集群管理器为驱动器启动申请的集群节点
5.驱动器程序将应用中的RDD转化和行动操作分发到各个执行器节点
6.在执行器中执行计算和结果的保存
7.驱动器的main方法退出后，驱动器将终止执行进程，回收资源
驱动器程序连接集群管理器，集群管理器分配资源
3.集群管理节点启动执行器节点
4.

